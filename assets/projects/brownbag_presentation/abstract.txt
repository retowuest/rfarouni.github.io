A Contemporary Overview of Latent Generative Models from a Probabilistic Perspective and an Application of a Deep Latent Gaussian Model to the Unsupervised Learning of Chromatin States from Functional Epigenomic Data

In this two-part talk, I will first present a brief overview of latent variable models from a probabilistic perspective. The main goal of the overview is to give a birds-eye view of the topographic structure of the space of latent variable models in light of recent developments in statistics and machine learning, that show - contrary to widely held conceptions- how seemingly unrealted models and methods are in fact intimitly related to each other. In the second part of the talk, I will present results from my work applying a Deep Latent Generative Model to high-dimensional, high-throughput functional epigenomics datasets with the goal of learning a latent representation of functional regions of the genome both across DNA sequence and across cell-types. Deep Latent Models bring together the strengths of two streams of statistical learning: deep learning and Bayesian probabilistic modeling. Whereas deep learning allows us to build scalable models that can capture the most complex nonlinear dependencies in the data, Bayesian generative modeling provides a probabilistic framework in which we can model, not just the variables we observe, but also latent variables that represent hidden patterns which can explain the observed data. 
