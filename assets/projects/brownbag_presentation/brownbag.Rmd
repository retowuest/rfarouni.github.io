---
title: "Brownbag Presentation"
author:
- "Rick Farouni"
date: March 20, 2017
output:
  revealjs::revealjs_presentation:
    theme: sky
    transition: slide
    center: true
    mathjax: local
    highlight: pygments
    fig_caption: true
    fig_width: 7
    fig_height: 6
    css: styles.css
    self_contained: false
    reveal_plugins: ["chalkboard", "notes", "zoom"]
    reveal_options:
      slideNumber: true
      previewLinks: true
bibliography: bibfile.bib
---

\newcommand{\btheta}{\boldsymbol{\theta}}
\newcommand{\bSigma}{\boldsymbol{\Sigma}}
\newcommand{\bepsilon}{\boldsymbol{\epsilon}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bz}{\mathbf{z}}
\newcommand{\bB}{\mathbf{B}}
\newcommand{\bD}{\mathbf{D}}
\newcommand{\bW}{\mathbf{W}}
\DeclareMathOperator{\Diag}{Diag}
\newcommand{\sN}{\mathcal{N}}
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```
# A Contemporary Overview of Latent Generative Models from a Probabilistic Perspective

***

## What is a Latent Variable?

![](lv.jpg)

## What is a Latent Variable Model?

$$
\by \in \mathbb{R}^{N \times P }; \by_n \in \mathbb{R}^{P}; \bz_n \in \mathbb{R}^{D}; \btheta \in \mathbb{R}^{M}\\
p(\by_1,\cdots,\by_N,\bz_1,\cdots,\bz_N,\btheta) = p(\btheta)\prod_{n=1}^N\ p(\by_n\mid\bz_n,\btheta)p(\bz_n\mid\btheta)$$
<hr>
<div style="width:600px; height:250px; margin:auto">
![Graphical Model Representation](lvm.jpg)
</div>

***

## Class of Latent Variable Models

>- Matrix factorization models (e.g. PCA, Factor Analysis)
>- Multilevel regression models (e.g. random effects model)
>- Time series models (e.g. Hidden Markov Model)
>- Dirichlet process mixture models
>- Deep latent variable models (e.g. VAE)
>- And others ...

***

## Multivariate Models

$$p(\by_1,\cdots, \by_N, \bx_1,\cdots, \bx_N, \theta)$$

$$\prod_{n=1}^Np(\by_n \mid f_{\theta}(\bx_n),\theta^{Y})p(\theta^{Y})\prod_{n=1}^N p( \bx_n \mid \theta^{X})  p(\theta^X)$$

## Multivariate Regression

$$\prod_{n=1}^N  \sN_p( \by_{n} \mid \bB \bx_n, \ \bSigma)$$

***

## Reduced Rank Regression

$$\textit{Let } \overset{p\times m}{\mathbf{B}}=\overset{p\times d}{\mathbf{W}}\ \overset{d\times m}{\mathbf{D}}\\
\prod_{n=1}^N  \sN_p( \by_n \mid \bW\bD\by_n, \ \bSigma)$$

<br>
<br>
<hr>
<small> @izenman2008 </small>


## Latent Variable Models

$$\textit{Let } {\bz_n}=\overset{d\times m}{\mathbf{D}}\overset{m\times 1}{\by_n}\\
\prod_{n=1}^N  \sN_p( \by_n \mid \bW\bz_n, \ \bSigma)$$

## Examples of Common Latent Variable Models



***

## Principle Component Analysis (PCA)

$$
\prod_{n=1}^N  \sN_p( \by_n \mid \bW\bz_n, \ \sigma^2\mathbf{I}) \mathcal{N}_d(\bz_n \mid \boldsymbol{0},\,\mathbf{I})
$$

### As a generative model 

$$
\bz_n\ \sim\ \mathcal{N}_d(\boldsymbol{0},\,\mathbf{I}) \quad  n=1,\cdots,N \\
\by_n \mid  \bz_n \sim\ \mathcal{N}_p(\bW\bz_n,\, \sigma^2\mathbf{I})
$$

<br>
<br>
<hr>
<small> @tipping1999 </small>


## Factor Analysis (FA)

$$
\prod_{n=1}^N  \sN_p( \by_n \mid \bW\bz_n, \ \Diag(\sigma^2)) \mathcal{N}_d(\bz_n \mid \boldsymbol{0},\,\mathbf{I})
$$

### As a generative model 

$$
\bz_n\ \sim\ \mathcal{N}_d(\boldsymbol{0},\,\mathbf{I}) \quad n=1,\cdots,N \\
\by_n \mid  \bz_n \sim\ \mathcal{N}_p(\bW\bz_n,\,\Diag(\sigma^2) )
$$

***

## Independent Component Analysis (ICA)

$$
\prod_{n=1}^N  \sN_p( \by_n \mid \bW\bz_n, \ \Diag(\sigma^2)) \prod_{n=1}^N \prod_{d=1}^D \mathcal{GG}_d(z_{d,n} \mid\alpha_d) \quad \alpha_d < 2 
$$ 

### As a generative model 

$$
\alpha_d < 2;\quad n=1,\cdots,N \\
z_{d,n}\ \sim\ \mathcal{GG}_d(\alpha_d) \quad d=1:D\\
\by_n \mid  \bz_n\sim\ \mathcal{N}_p(\bW\bz_n,\, \Diag(\sigma^2))
$$
<br>
<hr>
<small> @Hyvarinen2015 </small>


***

## Canonical Correlation Analysis (CCA)

$$
\bz_n \sim \mathcal{N}_d\left(\boldsymbol{0},\mathbf{I}\right) \quad n=1, \cdots,N\\
\begin{bmatrix}
    {\by_n}^{(1)}\mid \bz_n\\
    {\by_n}^{(2)}\mid \bz_n
\end{bmatrix} \sim \mathcal{N}_{(p1+p2)}\left(\begin{bmatrix}
    \mathbf{W}^{(1)}\\
    \mathbf{W}^{(2)} 
\end{bmatrix} \bz_n,\begin{bmatrix}
    \bSigma^{(1)} & \boldsymbol{0}\\
   \boldsymbol{0} & \bSigma^{(2)}
\end{bmatrix}\right) 
$$

<br>
<br>
<hr>
<small> @bach2005 </small>

***

## Deep Latent Gaussian Models (DLGM)

$$\mathbf{z}_n^{(L)}\ \sim\ \mathcal{N}_{d_{(L)}}(\boldsymbol{0},\, \mathbf{I}))  \quad n=1, \cdots,N \\
    \bz_n^{(l)}\ \sim\ \mathcal{N}_{d_{(l)}}(\bz_n^{(l)} \mid \mathbf{NN}^{(l)}(\bz_n^{(l+1)};\btheta^{(l)}),\, \Sigma^{(l)} )\quad l=1, \cdots,L-1\\
\by_n \mid  \bz_n \sim\ \mathbf{Expon}_p(\by_n \mid \mathbf{NN}^{(0)}(\bz_n^{(1)};\btheta^{(0)}))\\
$$
<hr>
$$ 
\textit{where } \quad
\mathbf{NN}(\bz;\theta) = h_K \circ h_{K-1} \circ \ldots \circ h_0(\mathbf{z}) \\
 \quad h_k(\by) = \sigma_k(\mathbf{W}^{(k)} \by+\mathbf{b}^{(k)})\\  
 \btheta = \{ (\mathbf{W}^{(k)}, \mathbf{b}^{(k)}) \}_{k=0}^K
$$
<hr>
<small> @rezende2014 </small>

***

## Variational Autoencoder (VAE)

<br>
<br>

$$\mathbf{z}_n\ \sim\ \mathcal{N}_d(\boldsymbol{0},\, \mathbf{I}))  \quad n=1, \cdots,N\\
\by_n \mid  \bz_n \sim\ \mathcal{N}_p(\mathbf{NN}_{\mu}(\bz_n;\btheta),\, \mathbf{NN}_{\sigma}(\mathbf{z}_n;\btheta)))
$$

<br>
<br>
<hr>
<small> @Kingma2013 </small>


***

### **Recent Extensions**

<small> - DRAW: deep recurrent attention writer (Gregor et al., 2015)</small>
<small> - VRNN: Variational recurrent neural network (Chung et al., 2015)</small>
<small> - DMM: Deep Markov models (Krishnan et al., 2016)</small>
<div style="width:600px; height:250px; margin:auto">
![Image Credit: Blei et. al. NIPS 2016 Tutorial Slides](posteriors.png)
</div>


# Application of a Deep Latent Gaussian Model to the Unsupervised Learning of Chromatin States

## Background

***

### The General Problem 

*Molecular phenotype* = $\Phi$( *genome, environment*)


<div style="width:1000px; height:400px; margin:auto">
![ ](phenotype.jpg)</div>

***

### Brief History 

>- Human Genome Project (2001) <small> Sequence all 3 billion base-pairs of the human genome (1.5% protein coding) </small>
>- ENCODE project (2012)  <small> Determine the functional role of the remaining 98.5% of the non-coding genome </small>
>- PsychENCODE project (2015) <small>How variation in functional genome is associated with psychiatric disorders </small>
>- Others Studies <small> How variation in the neuronal functional genome is associated with learning/plasticity and normal/abnormal cognition. See @rajarajan2016 for a review.</small>

***

### The Specific Computational Problem

*TF binding* = $\Phi_1$(*regulatory DNA*)

*Gene Expression* = $\Phi_2$(*TF binding*)

<div style="width:400px; height:200px; margin:auto">
![Central Dogma of Molecular Biology 2.0](DNAtoProtein.png)
</div>

***

###  Chromatin Architecture 

<div style="width:900px; height:500px; margin:auto">
![Chromatin Architecture and the Functional Genome](chromatin.jpg)
</div>

***

##  Data 

- A total of 100  ENCODE epigenomic datasets were used 
     - 10 ENCODE cell types
     - 10 ChIP-seq datasets (genome-wide signal coverage) 

***

###  Cell Types 

<div style="width:700px; height:500px; margin:auto">
![UCSC Genome Browser Tracks for the H3k27me3 mark for all 10 cells](1mark10cells.png)
</div>

***

###  Epigenetic Marks

<div style="width:500px; height:340px; margin:auto">
![UCSC Genome Browser Tracks for all 9 marks for GM12878 and H1-hESC cells](9marks2cells.png)
</div>

***

##  Preprocessing 

(1) Creating a blacklist file of excludable genomic regions
(2) Segment the human reference genome into 200bp bins
(3) Discard regions that overlap the blacklist
(4) Combine 100 bigWig signals into one data-frame
(5) Average the signal over the 200bp segments
(6) Subtract control signal from other 9 signals for each cell-type
(7) Normalize the signals
(8) Create labels from available functional annotation data

***

### Visualizing the Observations

<div style="width:1000px; height:800px; margin:auto">
![A Sample of 12 Observations from Test Data. Each has 10 cells (rows) x 9 Epigenentic Marks (columns)](observations_enhancers.png)
</div>



***

## Model: The Variational Autoencoder


<div style="width:500px; height:360px; margin:auto">
![VAE Model](vae.jpg)
</div>


***

## Generative Network

$$
p(\by,\bz) =\prod_{n=1}^N  \mathbf{Bernoulli}_p(\by_n \mid \mathbf{NN}(\bz_n;\btheta))\mathcal{N}_d(\bz_n \mid \boldsymbol{0},\,\mathbf{I})
$$
$$ 
\textit{where } \quad
\mathbf{NN}(\bz;\theta) = h_K \circ h_{K-1} \circ \ldots \circ h_0(\mathbf{z}) \\
 \quad h_k(\by) = \sigma_k(\mathbf{W}^{(k)} \by+\mathbf{b}^{(k)})\\  
 \btheta = \{ (\mathbf{W}^{(k)}, \mathbf{b}^{(k)}) \}_{k=0}^K\\
 \textit{is a function parameterized by a deep neural network}
$$

***

## Variational Approximation Network

Since the true posterior is intractable 
$$p_\theta(\mathbf{z}_n \mid \mathbf{y}_n) = \frac{p(\by_n \mid \bz_n)p(\bz_n)}{p(\by_n)}\\$$

We introduce an approximate posterior distribution
$$q_\nu(\mathbf{z}_n \mid \mathbf{y}_n;\nu) = \mathbf{Bernoulli}_p(\mathbf{NN}(\bz_n;\nu)))\\$$

***

And minimize Kullback–Leibler divergence between the approximate posterior and the true posterior

<div style="width:800px; height:500px; margin:auto">
![Image Credit: Blei et. al. NIPS 2016 Tutorial Slides](var_infer.jpg)
</div>

***

## Variational Inference for VAE

</br>

 > “Far better an approximate answer to the right question, which is often vague, than an exact answer to the wrong question, which can always be made precise.” - **John Tukey**
    

***

## Network Architecture

>- **Generative Model Network**
>     - Nonlinearities: *Input--> Relu--> Relu--> sigmoid*
>     - Layer Size: *2 --> 256--> 512--> 90*
>- **Variational Approximation Network**
>     - Nonlinearities: *Input ->Relu -->Relu --> (linear, softplus)*
>     - Layer Size: *90 -->512 -->256 -->(2, 2)*
>- Total Number of Parameters: 1,483,854   

<hr>
</br>
$$\Phi(\by, \theta) = \rho(W_L(\rho(W_{L-1} \cdots \rho(W_1(\by))\cdots)$$

<small> where $W_l$ is a linear operator and $\rho$ is a pointwise nonlinearity </small> 
 <hr>
 
## Nonlinearities

<div style="width:550px; height:500px; margin:auto">
![Activation Functions](activations.png)
</div>

***


## Inference

- Tensorflow via Python API
- Training Data:
    - All except Chromosomes 1, 8, and 21
    - Sample Size: 11,748,445
    - Dimensionality: 90
    - Validation Split: 80% Training, 20% Validation
- Test Data
    - Chromosomes 1, 8, and 21
    - Sample Size: 1,946,177 Observations
    
***


## Optimization

- Minibatch Stochastic Gradient Descent (SGD)
    - Nesterov Accelerated Adaptive Moment Estimation
    - Batch Size: 256
    - Number of Updates Per Epoch: 36,713
- Computational Cost: ~1 GFLOP per update  
<hr>
<div style="width:380px; height:280px; margin:auto">
![Learning Dynamics. Image credit: Alec Radford](opt2.gif)
</div>



***

### Projection of Validation Data onto Latent Space 

<div style="width:1000px; height:800px; margin:auto">
![Projection of 2,349,696 Observations into 2D Manifold](valid_unlabeled.png)
</div>

***

### Projection of Test Data onto Latent Space 

<div style="width:1000px; height:800px; margin:auto">
![Projection of 1,946,177 Observations into 2D Manifold](test_unlabeled.png)
</div>

***

### Functional Annotations 

- FANTOM5 Atlas of Active Enhancers (43,011 regions)
- CpG islands (52,502  regions)
- GENCODE Version 19 Gene Annotations: 
    - Coding DNA sequence (CDS)
    - Promoters
    

***

### Projected Validation Data (Labeled Subset)

<div style="width:1000px; height:800px; margin:auto">
![Projection of 116,851 Observations with Labels](valid_labeled.png)
</div>


***

### Projected Test Data (Labeled Subset)

<div style="width:1000px; height:800px; margin:auto">
![Projection of 102,115 Observations with Labels](test_labeled.png)
</div>

***

### Visualizing the Learned 2-D Manifold 

<div style="width:600px; height:600px; margin:auto">
![16 x 16 Grid of Samples from Latent Space](2dmanifold.png)
</div>


***


### Visualizing the Learned 2-D Manifold 

<div style="width:600px; height:600px; margin:auto">
![3 x 3 Close-up of Right Side of the Latent Space](2dmanifold_zoom.png)
</div>

***

### Reconstructing Observations

<div style="width:800px; height:800px; margin:auto">
![Reconstructions of Observations by Projection followed by Sampling from the Latent Manifold](reconstructions.png)
</div>

***


## Application

Better Unsupervised Learning Methods? 

- **ChromHMM** (Ernst J. and Kellis M, 2012): Hidden Markov Model
- **Segway** (Hoffman et al., 2012): Dynamic Bayesian Network

***


## Thank You!

</br>


> "Information has its own architecture. Each data source, whether imagery, sound, text, has an inner architecture which we should attempt to discover." - **David Donoho, Plenary Address at ICM 2012**
















## Extra

***

<div style="width:600px; height:600px; margin:auto">
![Learning the VAE Latent Space](vae.gif)
</div>

***

<div style="width:600px; height:600px; margin:auto">
![VAE's Approximate Posterior Latent Space (MNIST)](vae.png)
</div>

***

<div style="width:800px; height:800px; margin:auto">
![VAE's Approximate Posterior Latent Space (Faces)](vaefaces.jpg)
</div>


***



### Deep learning Models


<div style="width:900px; height:500px; margin:auto">
![Deep convolutional neural network (Mallat, 2016)](CNN.jpg)
</div>



***
### Why is Deep Learning succussful?

</br>
**Beats the curse of dimensionality!**
</br>
</br>

### How?

> 1. **Linearizes** intra-class variability while preserving inter-class variability 
> 2. **Regularizes** and incorporates prior information 
 <small> </br> ***Example***: A convolutional layer in a CNN imposes an infinitely strong prior that interactions are only local and equivariant to translation </small> 
 
 ***
 
### Learned Representations

 <div style="width:500px; height:400px; margin:auto">
![Linearization and Regularization (from ConvnetJS)](circleCNN.png)
</div>

***
 






